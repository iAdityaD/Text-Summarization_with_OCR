{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ea099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADITYA D\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\ADITYA D\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from skimage.filters import threshold_otsu\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import magic\n",
    "from PyPDF2 import PdfFileReader\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from heapq import *\n",
    "import warnings\n",
    " \n",
    "# Ignore all DeprecationWarnings, UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('ocr_processor')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('ocr_model')\n",
    "summarizer=pipeline(\"summarization\",model=\"text_summarizer\")\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578cea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_projections(sobel_image):\n",
    "    return np.sum(sobel_image, axis=1)  \n",
    "\n",
    "def sauvola_thresholding(image, window_size=25, k=0.4, R=128):\n",
    "    \n",
    "    # Convert image to float\n",
    "    image = image.astype(np.float64)\n",
    "    \n",
    "    # Calculate the mean and standard deviation for each window\n",
    "    mean = cv2.boxFilter(image, cv2.CV_64F, (window_size, window_size))\n",
    "    sq_mean = cv2.boxFilter(image**2, cv2.CV_64F, (window_size, window_size))\n",
    "    variance = sq_mean - mean**2\n",
    "    stddev = np.sqrt(variance)\n",
    "    \n",
    "    # Calculate Sauvola threshold\n",
    "    threshold = mean * (1 + k * ((stddev / R) - 1))\n",
    "    \n",
    "    # Apply threshold to binarize the image\n",
    "    binary_image = (image > threshold).astype(np.uint8) * 255\n",
    "    return 255-binary_image\n",
    "\n",
    "def heuristic(a, b):\n",
    "    return (b[0] - a[0]) ** 2 + (b[1] - a[1]) ** 2\n",
    "\n",
    "def astar(array, start, goal):\n",
    "    neighbors = [(0,1),(0,-1),(1,0),(-1,0),(1,1),(1,-1),(-1,1),(-1,-1)]\n",
    "    close_set = set()\n",
    "    came_from = {}\n",
    "    gscore = {start:0}\n",
    "    fscore = {start:heuristic(start, goal)}\n",
    "    oheap = []\n",
    "    heappush(oheap, (fscore[start], start))\n",
    "    while oheap:\n",
    "        current = heappop(oheap)[1]\n",
    "        if current == goal:\n",
    "            data = []\n",
    "            while current in came_from:\n",
    "                data.append(current)\n",
    "                current = came_from[current]\n",
    "            return data\n",
    "        close_set.add(current)\n",
    "        for i, j in neighbors:\n",
    "            neighbor = current[0] + i, current[1] + j            \n",
    "            tentative_g_score = gscore[current] + heuristic(current, neighbor)\n",
    "            if 0 <= neighbor[0] < array.shape[0]:\n",
    "                if 0 <= neighbor[1] < array.shape[1]:                \n",
    "                    if array[neighbor[0]][neighbor[1]] == 1:\n",
    "                        continue\n",
    "                else:\n",
    "                    # array bound y walls\n",
    "                    continue\n",
    "            else:\n",
    "                # array bound x walls\n",
    "                continue\n",
    "                \n",
    "            if neighbor in close_set and tentative_g_score >= gscore.get(neighbor, 0):\n",
    "                continue\n",
    "                \n",
    "            if  tentative_g_score < gscore.get(neighbor, 0) or neighbor not in [i[1]for i in oheap]:\n",
    "                came_from[neighbor] = current\n",
    "                gscore[neighbor] = tentative_g_score\n",
    "                fscore[neighbor] = tentative_g_score + heuristic(neighbor, goal)\n",
    "                heappush(oheap, (fscore[neighbor], neighbor))               \n",
    "    return []\n",
    "\n",
    "# find the midway where we can make a threshold and extract the peaks regions\n",
    "def find_peak_regions(hpp, threshold):\n",
    "    peaks = []\n",
    "    for i, hppv in enumerate(hpp):\n",
    "        if hppv < threshold:\n",
    "            peaks.append([i, hppv])\n",
    "    return peaks\n",
    "\n",
    "def get_binary(img):\n",
    "    mean = np.mean(img)\n",
    "    if mean == 0.0 or mean == 1.0:\n",
    "        return img\n",
    "\n",
    "    thresh = threshold_otsu(img)\n",
    "    binary = img <= thresh\n",
    "    binary = binary * 1\n",
    "    return binary\n",
    "\n",
    "def edges_detect(img_gray):\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "    # Sobel Edge Detection\n",
    "    sobelx = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=3)\n",
    "    sobely = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=3)\n",
    "    sobelxy = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=3)\n",
    "    edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200)\n",
    "    return edges\n",
    "\n",
    "def image_preprocess(img):\n",
    "    # check if its 3 channel or grayscale, based on that convert to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    #remove the noise\n",
    "    blurred_image = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    edges = edges_detect(blurred_image)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n",
    "    dilated_image = cv2.dilate(edges, kernel, iterations=1)\n",
    "\n",
    "    # Apply Sauvola thresholding\n",
    "    binary_image = sauvola_thresholding(dilated_image)\n",
    "    _, binary_image = cv2.threshold(binary_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    #edges = edges_detect(blurred_image)\n",
    "    #kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n",
    "    #dilated_image = cv2.dilate(edges, kernel, iterations=1)\n",
    "    \n",
    "    # Apply Sauvola thresholding\n",
    "    #_, binary_image = cv2.threshold(binary_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    return binary_image\n",
    "\n",
    "def extract_handwritten_text_images(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    original_img=img.copy()\n",
    "    binary_image = image_preprocess(img)  \n",
    "    hpp = horizontal_projections(binary_image)\n",
    "\n",
    "    # find the threshold from where anything above is considered a peak region\n",
    "    threshold =(np.max(hpp)-np.min(hpp))//2\n",
    "    peaks = find_peak_regions(hpp, threshold)\n",
    "\n",
    "    peaks_indexes = np.array(peaks)[:, 0].astype(int)\n",
    "\n",
    "    # group the peaks through which we will be doing path planning.\n",
    "    diff_between_consec_numbers = np.diff(peaks_indexes) \n",
    "\n",
    "    # difference between consecutive numbers\n",
    "    indexes_with_larger_diff = np.where(diff_between_consec_numbers > 1)[0].flatten()\n",
    "    peak_groups = np.split(peaks_indexes, indexes_with_larger_diff)\n",
    "\n",
    "    # remove very small regions, these are basically errors in algorithm because of our threshold value\n",
    "    peak_groups = [item for item in peak_groups if len(item) > 1]\n",
    "    \n",
    "    #binarize the image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    binary_image = get_binary(img)\n",
    "\n",
    "    # now that everything is cleaner, its time to segment all the lines using the A* algorithm\n",
    "    segment_separating_lines = []\n",
    "    for i, sub_image_index in enumerate(peak_groups):\n",
    "        nmap = binary_image[sub_image_index[0]:sub_image_index[-1]]\n",
    "        path = np.array(astar(nmap, (int(nmap.shape[0]/2), 0), (int(nmap.shape[0]/2),nmap.shape[1]-1)))\n",
    "        offset_from_top = sub_image_index[0]\n",
    "        path[:,0] += offset_from_top\n",
    "        segment_separating_lines.append(path)\n",
    "\n",
    "    # Lets divide the image now by the line segments passing through the image\n",
    "    first_line = int(np.max(segment_separating_lines[0][:,0]))\n",
    "    seperated_images = [original_img[:first_line]]\n",
    "    for index, line_segments in enumerate(segment_separating_lines):\n",
    "        if index < len(segment_separating_lines)-1:\n",
    "            lower_line =int(np.max(segment_separating_lines[index][:,0]))\n",
    "            upper_line = int(np.max(segment_separating_lines[index+1][:,0]))\n",
    "            seperated_images.append(original_img[lower_line:upper_line])\n",
    "    text=\"\"\n",
    "    for line_image in seperated_images:\n",
    "        text=text+\" \"+extract_handwritten(line_image)\n",
    "    return text\n",
    "\n",
    "def extract_handwritten(img):\n",
    "    pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    text=generated_text\n",
    "    return text\n",
    "\n",
    "#Extract pdf file consists of Typed text\n",
    "def extract_text_pdf(filepath, is_handwritten):\n",
    "    if is_handwritten:\n",
    "        return extract_handwritten_text_pdf(filepath)\n",
    "    else:\n",
    "        return extract_image_text_pdf(filepath)\n",
    "\n",
    "#Extract pdf file consists of scanned images\n",
    "def extract_image_text_pdf(filepath):\n",
    "    try:\n",
    "        doc = convert_from_path(filepath,poppler_path=r\"poppler-24.02.0\\Library\\bin\")\n",
    "        text=\"\"\n",
    "        for page_number, page_data in enumerate(doc):\n",
    "            text+=pytesseract.image_to_string(page_data)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return \"Error: Unable to extract text from pdf scanned document. \"+str(e)\n",
    "\n",
    "#Extract pdf file consists of handwritten text\n",
    "def extract_handwritten_text_pdf(filepath):\n",
    "    try:\n",
    "        images = convert_pdf_to_images(filepath,poppler_path=r\"poppler-24.02.0\\Library\\bin\")\n",
    "        text = \"\"\n",
    "        for img in images:\n",
    "            text += extract_handwritten_text_images(img)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return \"Error: Unable to extract text from pdf handwritten document. \"+str(e)\n",
    "\n",
    "def extract_text_docx(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'rb') as doc_file:\n",
    "            doc = Document(doc_file)\n",
    "            full_text = []\n",
    "            for paragraph in doc.paragraphs:\n",
    "                full_text.append(paragraph.text)\n",
    "            return \"\\n\".join(full_text).strip()\n",
    "    except Exception as e:\n",
    "        return \"Error: Unable to extract text from Word document. \"+ str(e)\n",
    "\n",
    "def extract_text_pptx(filepath):\n",
    "    try:\n",
    "        ppt = Presentation(filepath)\n",
    "        slides = ppt.slides\n",
    "        text = \"\"\n",
    "        for slide in slides:\n",
    "            shapes = slide.shapes\n",
    "            for shape in shapes:\n",
    "                if shape.has_text_frame:\n",
    "                    text_frame = shape.text_frame\n",
    "                    text += text_frame.text + \"\\n\"\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return \"Error: Unable to extract text from PowerPoint document. \"+ str(e)\n",
    "\n",
    "def extract_text_image(filename,is_handwritten):\n",
    "    try:\n",
    "        \n",
    "        if is_handwritten:\n",
    "            return extract_handwritten_text_images(filename)\n",
    "        else:\n",
    "            img = cv2.imread(filename)\n",
    "            return pytesseract.image_to_string(img)\n",
    "    except Exception as e:\n",
    "        return \"Error: Unable to extract text from image. \" +str(e) \n",
    "\n",
    "def convert_to_bullet_points(text):\n",
    "    sentences = text.split('. ')\n",
    "    bullet_points = sentences[:]\n",
    "    return '\\n'.join(f\"- {point.strip()}\" for point in bullet_points)\n",
    "\n",
    "def extract_text(filepath, is_handwritten):\n",
    "    mime = magic.Magic(mime=True)\n",
    "    mime_type = mime.from_file(filepath)\n",
    "\n",
    "    if mime_type == \"application/pdf\":\n",
    "        return extract_text_pdf(filepath, is_handwritten)\n",
    "    \n",
    "    elif mime_type in (\"application/msword\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"):\n",
    "        return extract_text_docx(filepath)\n",
    "    \n",
    "    elif mime_type in (\"application/vnd.ms-powerpoint\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"):\n",
    "        return extract_text_pptx(filepath)\n",
    "    \n",
    "    elif mime_type in (\"image/jpeg\", \"image/png\"):\n",
    "        return extract_text_image(filepath,is_handwritten)\n",
    "    \n",
    "    else:\n",
    "        return \"Error: Unsupported file type\"\n",
    "\n",
    "\n",
    "def summarize_text(file, is_handwritten, length):\n",
    "    text = extract_text(file.name, is_handwritten)\n",
    "\n",
    "    if not text or \"Error:\" in text:\n",
    "            return text\n",
    "\n",
    "    if not text.strip():\n",
    "        raise ValueError(\"Input text is empty. Please provide a valid text.\")\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Check if tokenization was successful\n",
    "    if not words:\n",
    "        raise ValueError(\"Tokenization failed or resulted in an empty list.\")\n",
    "    \n",
    "    # Count the words\n",
    "    word_count = len(words)\n",
    "    if word_count>50:\n",
    "        # Calculate max_length and min_length for the summary\n",
    "        max_length = int(word_count*(length/100))\n",
    "        min_length = int(max_length * 0.5)  # Ensure min_length is at least 1\n",
    "    else:\n",
    "        max_length = int(word_count)\n",
    "        min_length = int(max_length * 0.5)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary = summarizer(text, min_length=min_length, max_length=max_length)\n",
    "\n",
    "    # Extract the summarized text\n",
    "    summarized_text = summary[0]['summary_text']\n",
    "    return convert_to_bullet_points(summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3804248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://70464cf4e8dd3ffa49.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://70464cf4e8dd3ffa49.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft(),css=\"\"\".title {text-align: center; font-size: 24px; margin-bottom: 20px;} .description {text-align:left; font-size: 16px; margin-bottom: 20px;} .file-input-wrapper .gr-file {height: auto; max-height:20px;overflow: hidden;}\"\"\") as demo:\n",
    "    gr.HTML(\"\"\"<div class='title'>Notes Summarization</div>\"\"\")\n",
    "    gr.HTML(\"\"\"<div class='description'>\n",
    "    Follow the steps below to summarize your notes:<br>\n",
    "    1. Upload your file.<br>\n",
    "    2. Specify if it is handwritten.<br>\n",
    "    3. Choose the desired summary length.<br>\n",
    "    4. Click the 'Summarize' button and wait for a few minutes.</div>\"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload File\", file_types=[\"pdf\", \"docx\", \"pptx\", \"jpeg\", \"png\"],elem_id=\"file-input\" ,interactive=True)\n",
    "        with gr.Column():\n",
    "            is_handwritten = gr.Checkbox(label=\"Handwritten?\", interactive=True, info=\"is the doc contains handritten text.\")\n",
    "            length = gr.Slider(50, 100, step=10, label=\"Summary Length\", interactive=True)\n",
    "    \n",
    "    examples = gr.Examples(\n",
    "        examples=[\"Files/SOFTWARE ENGINEERING.pdf\",\"Files/Yolo.docx\", \"Files/tensor.pptx\"],\n",
    "        inputs=[file_input],\n",
    "        label=\"typed/scanned Examples\"\n",
    "    )\n",
    "    \n",
    "    examples = gr.Examples(\n",
    "        examples=[\n",
    "            [\"images/handwritten.jpg\", True]\n",
    "        ],\n",
    "        inputs=[file_input, is_handwritten],\n",
    "        label=\"handwritten Example\"\n",
    "    )\n",
    "        \n",
    "    summary_output = gr.Textbox(label=\"Summary\",lines=5,max_lines=20, interactive=False)\n",
    "    submit_button = gr.Button(\"Summarize\")\n",
    "    \n",
    "    submit_button.click(\n",
    "        summarize_text, \n",
    "        inputs=[file_input, is_handwritten, length], \n",
    "        outputs=summary_output\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e801daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95747755a3af4e4794a1eefde28e79c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0835a07b89e74665abe5f2256831cbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5214e64bd1554830871b7d0ccbf1ee2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489f60f096c24af8aa4caafaf4dde043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320c45c031fa46a3a218100dab498dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0c8d0739d6406199af1722416c5fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020d568455d34f31a74b1eb969a9e7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from transformers import pipeline\n",
    "#from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "#processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "#model =VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "#summarizer=pipeline(\"summarization\",model=\"pszemraj/led-base-book-summary\")\n",
    "\n",
    "#processor.save_pretrained(\"ocr_processor\")\n",
    "#model.save_pretrained(\"ocr_model\")\n",
    "#summarizer.save_pretrained(\"text_summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708125b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
